# Mule Runtime High Availability (HA) Cluster Overview

A **cluster** is a set of Mule runtimes that acts as a unit. In other words, a cluster is a virtual server composed of multiple nodes. The nodes (Mule runtimes) in a cluster communicate and share information through a distributed shared memory grid. This means that the data is replicated across memory in different physical machines.

![cluster](https://docs.mulesoft.com/mule-runtime/3.9/_images/cluster.png)

Contact your customer service representative about pricing for this feature.

## The Benefits of Clustering

By default, clustering Mule runtimes ensures high system availability. (If you wish to use clusters for high performance instead, see [Clustering for High Performance](https://docs.mulesoft.com/mule-runtime/3.9/mule-high-availability-ha-clusters#clustering-for-high-performance) below.) If a Mule runtime node becomes unavailable due to failure or planned downtime, another node in the cluster can assume the workload and continue to process existing events and messages. The following figure illustrates the processing of incoming messages by a cluster of two nodes. Notice that the processing load is balanced across nodes – Node 1 processes message 1 while Node 2 simultaneously processes message 2.

![FailoverNoFail](https://docs.mulesoft.com/mule-runtime/3.9/_images/failovernofail.png)

If one node fails, the other available nodes pick up the work of the failing node. As shown in the following figure, if Node 2 fails, Node 1 processes both message 1 and message 2.

![FailoverNode2Fail](https://docs.mulesoft.com/mule-runtime/3.9/_images/failovernode2fail.png)

Because all nodes in a cluster of Mule runtimes process messages simultaneously, clusters can also improve performance and scalability. Compared to a single node instance, clusters can support more users or improve application performance by sharing the workload across multiple nodes or by adding nodes to the cluster.

The following figure illustrates workload sharing in more detail. Both nodes process messages related to order fulfillment. However, when one node is heavily loaded, it can move the processing for one or more steps in the process to another node. Here, processing of the Process order discount step is moved to Node 1, and processing of the Fulfill order step is moved to Node 2.

![cluster diagram](https://docs.mulesoft.com/mule-runtime/3.9/_images/cluster-diagram.png)

Beyond benefits such as high availability through automatic failover, improved performance, and enhanced scalability, clustering Mule runtimes offers the following benefits:

- Automatic coordination of access to resources such as files, databases, and FTP sources. The Mule runtime cluster automatically manages which node (Mule runtime) will handle communication from a data source.
- Automatic load balancing of processing within a cluster. If you divide your flows into a series of steps and connect these steps with a transport such as VM, each step is put in a queue, making it cluster enabled. The cluster of Mule runtimes can then process each step in any node, and so better balance the load across nodes.
- Raised alerts. You can set up an alert to appear when a node goes down and when a node comes back up.

All Mule runtimes in a cluster actively process messages. Note that each Mule runtime is also internally scalable – a single Mule runtime can scale easily by taking advantage of multiple cores. Even when a Mule runtime takes advantage of multiple cores, it still operates as a single node (Mule runtime) in a cluster.

### Concurrency Issues Solved by Clusters

The following problems may exist when you have a server group composed of multiple servers that **aren’t binded as a cluster**. You don’t have to worry about any of them if you group your servers as a cluster:

- **File based transports**: All Mule instances access the same mule file folders concurrently, which can lead to duplicate file processing and even possible failures if a file is deleted or modified by the Mule application
- **Multicast transport**: All mule instances get the same TCP requests and then process duplicate messages
- **JMS Topics**: All mule instances connect to the same JMS topic, which may lead to repeated processing of messages when scaling the non clustered Mule instance horizontally
- **JMS request-reply/request-response**: All Mule instances are listening for messages in the same response queue, this implies that a Mule instance might obtain a response that isn’t correlated to the request it sent. This can result in incorrect responses or make a flow fail with timeout.
- **Idempotent-redelivery-policy**: Idempotency doesn’t work if the same request is received by different Mule instances. Duplicated messages aren’t possible
- **Salesforce streaming API**: if multiple instances of the same application are deployed, they will fail since the API only supports a single consumer. No failover support in case the instance connected is stopped or crashes

## About Clustering

All the Mule runtimes in a cluster group together to form a single unit. Thus, you can deploy, monitor, or stop all the Mule runtimes in a cluster as if they were a single Mule runtime.

All the Mule runtimes in a cluster share memory, as illustrated below:

![topology_4-cluster](https://docs.mulesoft.com/mule-runtime/3.9/_images/topology-4-cluster.png)

Mule uses an **active-active** model to cluster Mule runtimes, rather than an **active-passive** model.

In an **active-passive** model, one node in a cluster acts as the **primary**, or active node, while the others are **secondary**, or passive nodes. The application in such a model runs on the primary node, and only ever runs on the secondary node if the first one fails. In this model, the processing power of the secondary node(s) is mostly wasted in passive waiting for the primary node to fail.

In an **active-active** model, no one node in the cluster acts as the primary node; all nodes in the cluster support the application. This application in this model runs on all the nodes, even splitting apart message processing between nodes to expedite processing across nodes.

## About Queues

You can set up a **VM queue** explicitly to **load balance** across Mule runtimes (nodes). Thus, if your entire application flow contains a sequence of child flows, the cluster can assign each successive child flow to whichever Mule runtime (node) happens to be available at the time. Potentially, the cluster can process a single message on multiple nodes as it passes through the VM endpoints in the application flow, as illustrated below:

![load_balancing](https://docs.mulesoft.com/mule-runtime/3.9/_images/load-balancing.png)

## About High-Reliability Applications

A **high-reliability** application must meet the following requirements:

1. Zero tolerance for message loss
2. A reliable underlying enterprise service bus (Mule)
3. Highly reliable individual connections

The feature known as **transactionality** tracks application event sequences to ensure that each message-processing step gets completed successfully, and therefore, no messages get lost or processed incorrectly. If a step fails, for some reason, the transactional mechanism rolls back all previous processing events, then restarts the message processing sequence again.

Transports such as JMS, VM, and JDBC provide built-in transactional support, thus ensuring that messages get processed reliably. For example, you can configure a transaction on an inbound JMS connection endpoint to remove messages from the JMS server only after the transaction has been committed. This ensures that the original message remains available for reprocessing if an error occurs during the processing flow.

You must use **XA transactions** to move messages between dissimilar transports that support transactions. This ensures that the Mule runtime commits associated transactions from all the dissimilar transports as a single unit. See [Transaction Management](https://docs.mulesoft.com/mule-runtime/3.9/transaction-management) for more information on XA and other types of transactions supported by Mule runtimes.

## Cluster Support for Transports

All Mule transports are supported within a cluster. Because of differences in the way different transports access inbound traffic, the details of this support vary. In general, outbound traffic acts the same way inside and outside a cluster – the differences are highlighted below.

Mule runtimes support three basic types of transports:

- Socket-based transports read input sent to network sockets that Mule owns. Examples include TCP, UDP, and HTTP[S].
- listener-based transports read data using a protocol that fully supports concurrent multiple accessors. Examples include JMS and VM.
- resource-based transports read data from a resource that allows multiple concurrent accessors, but does not natively coordinate their use of the resource. For instance, suppose multiple programs are processing files in the same shared directory by reading, processing, and then deleting the files. These programs must use an explicit, application-level locking strategy to prevent the same file from being processed more than once. Examples of resource-based transports include File, FTP, SFTP, E-mail, and JDBC.

All three basic types of transports are supported in clusters in different ways, as described below.

- Socket-based
  - Since each clustered Mule runtime runs on a different network node, each instance receives only the socket-based traffic sent to its node. Incoming socket-based traffic should be [Clustering and Load Balancing](https://docs.mulesoft.com/mule-runtime/3.9/mule-high-availability-ha-clusters#clustering-and-load-balancing) to distribute it among the clustered instances.
  - Output to socket-based transports is written to a specific host/port combination. If the host/port combination is an external host, no special considerations apply. If it is a port on the local host, consider using that port on the load balancer instead to better distribute traffic among the cluster.
- Listener-based
  - Listener-based transports fully support multiple readers and writers. No special considerations apply either to input or to output.
  - Note that, in a cluster, VM transport queues are a shared, cluster-wide resource. The cluster will automatically synchronize access to the VM transport queues. Because of this, a message written to a VM queue can be processed by any cluster node. This makes VM ideal for sharing work among cluster nodes.
- Resource-based
  - Mule HA Clustering automatically coordinates access to each resource, ensuring that only one clustered instance accesses each resource at a time. Because of this, it is generally a good idea to immediately write messages read from a resource-based transport to VM queues. This allows the other cluster nodes to take part in processing the messages.
  - There are no special considerations in writing to resource-based clustered transports:
    - When writing to file-based transports (File, FTP, SFTP), Mule will generate unique file names.
    - When writing to JDBC, Mule can generate unique keys.
    - Writing e-mail is effectively listener-based rather than resource-based.

## Clustering and Reliable Applications

High-reliability applications (ones that have zero tolerance for message loss) not only require the underlying Mule to be reliable, but that reliability needs to extend to individual connections. [Reliability Patterns](https://docs.mulesoft.com/mule-runtime/3.9/reliability-patterns) give you the tools to build fully reliable applications in your clusters.

Current Mule documentation provides [code examples](https://docs.mulesoft.com/mule-runtime/3.9/reliability-patterns) that show how you can implement a reliability pattern for a number of different non-transactional transports, including HTTP, FTP, File, and IMAP. If your application uses a non-transactional transport, follow the reliability pattern. These patterns ensure that a message is accepted and successfully processed or that it generates an "unsuccessful" response allowing the client to retry.

If your application uses transactional transports, such as JMS, VM, and JDBC, use transactions. Mule’s built-in support for transactional transports enables reliable messaging for applications that use these transports.

These actions can also apply to non-clustered applications.

## Clustering and Networking

### Single Data-Center Clustering

To ensure reliable connectivity between cluster nodes, all nodes of a cluster should be located on the same LAN. Implementing a cluster with nodes across geographically separated locations, such as different data centers that are connected through a VPN, is possible but not recommended.

Ensuring that all cluster nodes reside on the same LAN is the best practice to lower the possibility of network interruptions and unintended consequences, such as duplicated messages.

### Distributed Data-center Clustering

Linking cluster nodes through a WAN network introduces many possible points of failure, such as external routers and firewalls, which can prevent proper synchronization between cluster nodes. This not only affects performance but also requires you to plan for possible side effects in your app. For example, when two cluster nodes reconnect after getting cut off by a failed network link, the ensuing synchronization process can cause messages to be processed twice, which creates duplicates that must be handled in your application logic.

Note that it is possible to use nodes of a cluster located in different data centers and not necessarly located on the same LAN, but some restrictions apply.

To prevent this behavior, it is necessary to enable the Quorum Protocol. This protocol is used to allow one set of nodes to continue processing data while other sets do nothing with the shared data until they reconnect. Basically, when a disconnection occurs, only the portion with the most nodes will continue to function. For instance, assume two data centers, one with three nodes and another with two nodes. If a connectivity problem occurs in the data center with two nodes, then the data center with three nodes will continue to function, and the second data center will not. If the three-noded data center goes offline, none of your nodes will function. To prevent this outage, you must create the cluster in at least three data centers with the same number of nodes. It is unlikely for two data centers to crash, so if just one data center goes offline, the cluster will always be functional.

A cluster partition that does not have enough nodes to function will continue reacting to external system calls, but all operations over the object stores will fail, preventing data generation.

Limitations

- Quorum is only supported in Object Store-related operations.
- Distributed locking is not supported, which affects:
  - File/FTP transport polling for files concurrent
  - Idempotent Redelivery Policy component
  - Idempotent Message Filter component
- In-memory messaging is not suported, which affects:
  - VM transport
  - SEDA queues
- The Quorum feature can only be configured manually. See [Creating and Managing a Cluster Manually](https://docs.mulesoft.com/mule-runtime/3.9/creating-and-managing-a-cluster-manually)

## Clustering and Load Balancing

When Mule clusters are used to serve TCP requests (where TCP includes SSL/TLS, UDP, Multicast, HTTP, and HTTPS), some load balancing is needed to distribute the requests among the clustered instances. There are various software load balancers available, two of them are:

- Nginx, an open-source HTTP server and reverse proxy. You can use nginx’s [HttpUpstreamModule](http://wiki.nginx.org/HttpUpstreamModule)for HTTP(S) load balancing. You can find further information in the Linode Library entry [Use Nginx for Proxy Services and Software Load Balancing](http://library.linode.com/web-servers/nginx/configuration/front-end-proxy-and-software-load-balancing).
- The Apache web server, which can also be used as an HTTP(S) load balancer.

There are also many hardware load balancers that can route both TCP and HTTP(S) traffic.

## Clustering for High Performance

High performance is implemented differently on [CloudHub](https://docs.mulesoft.com/runtime-manager/) and [Pivotal Cloud Foundry](https://docs.mulesoft.com/runtime-manager/deploying-to-pcf), so this section applies only for [on-premises deployments](https://docs.mulesoft.com/runtime-manager/deploying-to-your-own-servers).

If high performance is your primary goal (rather than reliability), you can configure a Mule cluster or an individual application for maximum performance using a **performance profile**. By implementing the performance profile for specific applications within a cluster, you can maximize the scalability of your deployments while deploying applications with different performance and reliability requirements in the same cluster. By implementing the performance profile at the container level, you apply it to all applications within that container. Application-level configuration overrides container-level configuration.

Setting the performance profile has two effects:

- It disables distributed queues, using local queues instead to prevent data serialization/deserialization and distribution in the shared data grid.
- It implements the object store without backups, to avoid replication.

To configure the performance profile at the *container* level, add to **mule-cluster.properties** or to the system properties from the command line or wrapper.conf:

```
mule.cluster.storeprofile=performance
```

To configure the performance profile at the *individual application* level, add the profile inside a configuration wrapper, as shown below.

**Performance Store Profile**

```xml
<mule>
   <configuration>
      <cluster:cluster-config>
         <cluster:performance-store-profile/>
      </cluster:cluster-config>
   </configuration>
</mule>
```

Remember that application-level configuration overrides container-level configuration. If you would like to configure the container for high performance but make one ore more individual applications within that container prioritize reliability, include the following code in those applications:

**Reliable Store Profile**

```xml
<mule>
    <configuration>
        <cluster:cluster-config>
            <cluster:reliable-store-profile/>
        </cluster:cluster-config>
    </configuration>
</mule>
```

In cases of high load with endpoints that do not support load balancing, applying the performance profile may degrade performance. If you are using a File-based transport with an asynchronous processing strategy, JMS topics, multicasting, or HTTP connectors without a load balancer, the high volume of messages entering a single node can cause bottlenecks, and thus it can be better for performance to turn off the performance profile for these applications.

You can also choose to define a minimum number of machines required in a cluster for it to remain in an operational state. This grants you a consistency improvement. Find more information in our [quorum management section](https://docs.mulesoft.com/mule-runtime/3.9/creating-and-managing-a-cluster-manually#quorum-management).

## HA Cluster Demo

To evaluate Mule’s HA clustering capabilities first-hand, continue on to the [Mule HA Demo](https://docs.mulesoft.com/mule-runtime/3.9/evaluating-mule-high-availability-clusters-demo). Designed to help new users evaluate the capabilities of Mule High Availability Clusters, the Mule HA Demo Bundle teaches you how to use the Mule Management Console to create a cluster of Mule runtimes, then deploy an application to run on the cluster. Further, this demo simulates two processing scenarios that illustrate the cluster’s ability to automatically balance normal processing load, and its ability to reliably remain active in a failover situation.

## Best Practices

There are a number of recommended practices related to clustering. These include:

- As much as possible, organize your application into a series of steps where each step moves the message from one transactional store to another.
- If your application processes messages from a non-transactional transport, use a [reliability pattern](https://docs.mulesoft.com/mule-runtime/3.9/reliability-patterns) to move them to a transactional store such as a VM or JMS store.
- Use transactions to process messages from a transactional transport. This ensures that if an error is encountered, the message reprocesses.
- Use distributed stores such as those used with the VM or JMS transport – these stores are available to an entire cluster. This is preferable to the non-distributed stores used with transports such as File, FTP, and JDBC – these stores are read by a single node at a time.
- Use the VM transport to get optimal performance. Use the JMS transport for applications where data needs to be saved after the entire cluster exits.
- Create the number of nodes within a cluster that best meets your needs.
- Implement [reliability patterns](https://docs.mulesoft.com/mule-runtime/3.9/reliability-patterns) to create high reliability applications.

## Prerequisites and Limitations

- To date, MuleSoft has only tested eight node clusters. If you plan to scale beyond eight nodes. If you plan to use many nodes in your cluster, or you plan to store large message payloads, it is recommended that you research Hazelcast cluster scaling in the [Hazelcast documentation](https://hazelcast.org/documentation/), or to contact [MuleSoft Support](http://support.mulesoft.com/)
- You must have at least two Mule runtimes in a cluster, each of which should run on different physical (or virtual) machines.
- To maintain synchronization between the nodes in the cluster, Mule HA requires a reliable network connection between servers.
- You must keep the following ports open in order to set up a Mule cluster: port 5701 and port 54327.
- Because new cluster member discovery is performed using multicast, you need to enable the multicast IP: 224.2.2.3
- To serve TCP requests, some load balancing across a Mule cluster is needed. See [Clustering and Load Balancing](https://docs.mulesoft.com/mule-runtime/3.9/mule-high-availability-ha-clusters#clustering-and-load-balancing) for more information about third-party load balancers that you can use. You can also load balance the processing within a cluster by separating your flows into a series of steps and connecting each step with a transport such as VM. This cluster enables each step, allowing Mule to better balance the load across nodes.
- If your [custom message source](https://docs.mulesoft.com/mule-runtime/3.9/endpoint-configuration-reference) does not use a message receiver to define node [polling](http://en.wikipedia.org/wiki/Polling_(computer_science)), then you must configure your message source to implement a ClusterizableMessageSource interface.
  ClusterizableMessageSource dictates that only one application node inside a cluster contains the active (that is, started) instance of the message source; this is the ACTIVE node. If the active node falters, the ClusterizableMessageSource selects a new active node, then starts the message source in that node.

## See Also

- [Install an Enterprise License](https://docs.mulesoft.com/mule-runtime/3.9/installing-an-enterprise-license) to start managing clusters in production.
- For instructions on how to create and manage a cluster, see [Managing Servers](https://docs.mulesoft.com/runtime-manager/managing-servers#create-a-cluster).







# Mule运行时高可用性（HA）群集概述

甲**簇**是一组骡运行时的作用为一个单元。换句话说，集群是由多个节点组成的虚拟服务器。集群中的节点（Mule运行时）通过分布式共享内存网格进行通信和共享信息。这意味着数据在不同物理机器的内存中复制。

![簇](https://docs.mulesoft.com/mule-runtime/3.9/_images/cluster.png)

有关此功能的定价，请与您的客户服务代表联系。

## 集群的好处

默认情况下，群集Mule运行时可确保高系统可用性。（如果您希望使用群集来实现高性能，请参阅下面的[群集以获得高性能](https://docs.mulesoft.com/mule-runtime/3.9/mule-high-availability-ha-clusters#clustering-for-high-performance)。）如果Mule运行时节点由于故障或计划停机而变得不可用，则群集中的另一个节点可以承担工作负载并继续处理现有事件和消息。下图说明了由两个节点的集群处理传入消息。请注意，处理负载在节点之间平衡 - 节点1处理消息1，而节点2同时处理消息2。

![FailoverNoFail](https://docs.mulesoft.com/mule-runtime/3.9/_images/failovernofail.png)

如果一个节点发生故障，其他可用节点将获取故障节点的工作。如下图所示，如果节点2出现故障，则节点1将同时处理消息1和消息2。

![FailoverNode2Fail](https://docs.mulesoft.com/mule-runtime/3.9/_images/failovernode2fail.png)

由于Mule运行时集群中的所有节点同时处理消息，因此集群还可以提高性能和可伸缩性。与单个节点实例相比，群集可以通过跨多个节点共享工作负载或通过向群集添加节点来支持更多用户或提高应用程序性能。

下图更详细地说明了工作负载共享。两个节点都处理与订单履行相关的消息。但是，当一个节点负载很重时，它可以将进程中的一个或多个步骤的处理移动到另一个节点。这里，处理订单折扣步骤的处理被移动到节点1，并且执行订单步骤的处理被移动到节点2。

![集群图](https://docs.mulesoft.com/mule-runtime/3.9/_images/cluster-diagram.png)

除了通过自动故障转移实现高可用性，提高性能和增强可扩展性等优势之外，群集Mule运行时还具有以下优势：

- 自动协调对文件，数据库和FTP源等资源的访问。Mule运行时集群自动管理哪个节点（Mule运行时）将处理来自数据源的通信。
- 集群内处理的自动负载平衡。如果将流划分为一系列步骤并将这些步骤与VM等传输连接起来，则会将每个步骤放入队列中，使其启用集群。然后，Mule运行时集群可以处理任何节点中的每个步骤，从而更好地平衡节点之间的负载。
- 提出警报。您可以设置警报，以便在节点关闭和节点重新启动时显示。

群集中的所有Mule运行时都会主动处理消息。请注意，每个Mule运行时也是内部可伸缩的 - 单个Mule运行时可以通过利用多个核心轻松扩展。即使Mule运行时利用多个内核，它仍然作为集群中的单个节点（Mule运行时）运行。

### 群集解决的并发问题

如果服务器组由多个**未绑定为群集**的服务器组成，则可能存在以下问题。如果将服务器组合为一个集群，则不必担心它们中的任何一个：

- **基于文件的传输**：所有Mule实例同时访问相同的mule文件夹，如果文件被Mule应用程序删除或修改，可能导致文件处理重复甚至可能出现故障
- **多播传输**：所有mule实例获得相同的TCP请求，然后处理重复的消息
- **JMS主题**：所有mule实例都连接到相同的JMS主题，这可能导致在水平扩展非群集Mule实例时重复处理消息
- **JMS请求 - 回复/请求 - 响应**：所有Mule实例都在侦听同一响应队列中的消息，这意味着Mule实例可能获得与其发送的请求无关的响应。这可能导致错误响应或使流程因超时而失败。
- **幂等重新发送策略**：如果不同的Mule实例接收到相同的请求，则幂等性不起作用。重复的消息是不可能的
- **Salesforce流API**：如果部署了同一应用程序的多个实例，则它们将失败，因为API仅支持单个使用者。如果连接的实例已停止或崩溃，则无故障转移支持

## 关于群集

群集组中的所有Mule运行时一起形成一个单元。因此，您可以在群集中部署，监视或停止所有Mule运行时，就好像它们是单个Mule运行时一样。

群集中的所有Mule运行时共享内存，如下所示：

![topology_4集群](https://docs.mulesoft.com/mule-runtime/3.9/_images/topology-4-cluster.png)

Mule使用**主动 - 主动**模型来聚类Mule运行时，而不是**主动 - 被动**模型。

在**主动 - 被动**模型中，群集中的一个节点充当**主**节点或主动节点，而其他节点则是**辅助**节点或被动节点。此类模型中的应用程序在主节点上运行，并且只有在第一个节点发生故障时才在辅助节点上运行。在该模型中，辅助节点的处理能力主要在被动等待主节点失败时被浪费。

在**主动 - 主动**模型中，群集中没有一个节点充当主节点; 集群中的所有节点都支持该应用程序。此模型中的此应用程序在所有节点上运行，甚至在节点之间拆分消息处理以加速跨节点的处理。

## 关于队列

您可以显式设置**VM队列**以跨Mule运行时（节点）进行**负载平衡**。因此，如果整个应用程序流包含一系列子流，则集群可以将每个连续的子流分配给当时恰好可用的Mule运行时（节点）。当集群通过应用程序流中的VM端点时，集群可以在多个节点上处理单个消息，如下所示：

![负载均衡](https://docs.mulesoft.com/mule-runtime/3.9/_images/load-balancing.png)

## 关于高可靠性应用

一个**高可靠性**的应用程序必须满足以下要求：

1. 消息丢失零容忍
2. 可靠的底层企业服务总线（Mule）
3. 高度可靠的个人连接

称为**事务性**的功能跟踪应用程序事件序列，以确保每个消息处理步骤都成功完成，因此，不会丢失或错误处理消息。如果步骤失败，由于某种原因，事务机制将回滚所有先前的处理事件，然后再次重新启动消息处理序列。

JMS，VM和JDBC等传输提供内置的事务支持，从而确保可靠地处理消息。例如，您可以在入站JMS连接端点上配置事务，以便仅在提交事务后从JMS服务器中删除消息。这可确保在处理流程中发生错误时，原始消息仍可用于重新处理。

您必须使用**XA事务**在支持事务的不同传输之间移动消息。这可确保Mule运行时将来自所有不同传输的关联事务作为单个单元提交。有关Xule和Mule运行时支持的其他类型[事务](https://docs.mulesoft.com/mule-runtime/3.9/transaction-management)的更多信息，请参阅[事务管理](https://docs.mulesoft.com/mule-runtime/3.9/transaction-management)。

## 运输的集群支持

群集中支持所有Mule传输。由于不同传输方式访问入站流量的方式不同，因此此支持的详细信息会有所不同。通常，出站流量在群集内部和外部的行为方式相同 - 差异在下面突出显示。

Mule运行时支持三种基本类型的传输：

- 基于套接字的传输读取发送到Mule拥有的网络套接字的输入。示例包括TCP，UDP和HTTP [S]。
- 基于侦听器的传输使用完全支持并发多个访问器的协议读取数据。示例包括JMS和VM。
- 基于资源的传输从允许多个并发访问器的资源中读取数据，但本身不协调它们对资源的使用。例如，假设多个程序通过读取，处理然后删除文件来处理同一共享目录中的文件。这些程序必须使用显式的应用程序级锁定策略，以防止同一文件被多次处理。基于资源的传输示例包括文件，FTP，SFTP，电子邮件和JDBC。

如下所述，所有三种基本类型的传输以不同方式支持成簇。

- 插座为主
  - 由于每个群集Mule运行时在不同的网络节点上运行，因此每个实例仅接收发送到其节点的基于套接字的流量。传入的基于套接字的流量应该是[群集和负载平衡，](https://docs.mulesoft.com/mule-runtime/3.9/mule-high-availability-ha-clusters#clustering-and-load-balancing)以便在群集实例之间分发它。
  - 基于套接字的传输的输出被写入特定的主机/端口组合。如果主机/端口组合是外部主机，则不需要特殊注意事项。如果它是本地主机上的端口，请考虑在负载均衡器上使用该端口，以便更好地在群集之间分配流量。
- 侦听器为基础的
  - 基于侦听器的传输完全支持多个读者和编写者。输入或输出均无特殊注意事项。
  - 请注意，在群集中，VM传输队列是共享的群集范围资源。群集将自动同步对VM传输队列的访问。因此，任何群集节点都可以处理写入VM队列的消息。这使VM非常适合在群集节点之间共享工作。
- 资源型
  - Mule HA Clustering自动协调对每个资源的访问，确保一次只有一个群集实例访问每个资源。因此，立即将从基于资源的传输读取的消息写入VM队列通常是个好主意。这允许其他群集节点参与处理消息。
  - 写入基于资源的集群传输时没有特别注意事项：
    - 在写入基于文件的传输（文件，FTP，SFTP）时，Mule将生成唯一的文件名。
    - 写入JDBC时，Mule可以生成唯一键。
    - 编写电子邮件实际上是基于侦听器而不是基于资源的。

## 集群和可靠的应用程序

高可靠性应用程序（对消息丢失具有零容忍度）不仅要求底层Mule可靠，而且可靠性需要扩展到各个连接。[可靠性模式](https://docs.mulesoft.com/mule-runtime/3.9/reliability-patterns)为您提供了在群集中构建完全可靠应用程序的工具。

当前的Mule文档提供了[代码示例](https://docs.mulesoft.com/mule-runtime/3.9/reliability-patterns)，说明如何为许多不同的非事务传输实现可靠性模式，包括HTTP，FTP，文件和IMAP。如果您的应用程序使用非事务性传输，请遵循可靠性模式。这些模式确保消息被接受并成功处理，或者它生成“不成功”响应，允许客户端重试。

如果您的应用程序使用事务传输（例如JMS，VM和JDBC），请使用事务。Mule对事务传输的内置支持可为使用这些传输的应用程序提供可靠的消息传递。

这些操作也适用于非群集应用程序。

## 集群和网络

### 单数据中心群集

为确保群集节点之间的可靠连接，群集的所有节点都应位于同一LAN上。可以实现跨地理位置分离的节点实现群集，例如通过VPN连接的不同数据中心，但不建议这样做。

确保所有群集节点驻留在同一LAN上是降低网络中断和意外后果（如重复消息）的可能性的最佳实践。

### 分布式数据中心集群

通过WAN网络链接群集节点会引入许多可能的故障点，例如外部路由器和防火墙，这可能会阻止群集节点之间的正确同步。这不仅会影响性能，还需要您在应用中规划可能的副作用。例如，当两个群集节点在被失败的网络链接切断后重新连接时，随后的同步过程可能导致消息被处理两次，从而产生必须在应用程序逻辑中处理的重复项。

请注意，可以使用位于不同数据中心的群集节点，而不必位于同一LAN上，但有一些限制。

要防止此行为，必须启用Quorum协议。此协议用于允许一组节点继续处理数据，而其他集合对共享数据不执行任何操作，直到它们重新连接为止。基本上，当发生断开连接时，只有节点最多的部分才会继续工作。例如，假设有两个数据中心，一个有三个节点，另一个有两个节点。如果具有两个节点的数据中心发生连接问题，则具有三个节点的数据中心将继续运行，而第二个数据中心则不会。如果三节点数据中心脱机，则任何节点都不会运行。要防止此中断，必须在至少三个具有相同节点数的数据中心中创建群集。两个数据中心不太可能崩溃，

没有足够节点运行的集群分区将继续对外部系统调用做出反应，但对象存储上的所有操作都将失败，从而阻止数据生成。

限制

- Quorum仅在与Object Store相关的操作中受支持。
- 不支持分布式锁定，这会影响：
  - 文件/ FTP传输轮询文件并发
  - 幂等重新交付政策组件
  - 幂等消息过滤器组件
- 不支持内存中消息传递，这会影响：
  - VM传输
  - SEDA排队
- Quorum功能只能手动配置。请参阅[手动创建和管理群集](https://docs.mulesoft.com/mule-runtime/3.9/creating-and-managing-a-cluster-manually)

## 群集和负载平衡

当Mule集群用于提供TCP请求（其中TCP包括SSL / TLS，UDP，多播，HTTP和HTTPS）时，需要一些负载平衡来在集群实例之间分发请求。有各种软件负载平衡器可用，其中两个是：

- Nginx，一个开源HTTP服务器和反向代理。您可以使用nginx的[HttpUpstreamModule](http://wiki.nginx.org/HttpUpstreamModule)进行HTTP（S）负载平衡。您可以在Linode Library条目中找到更多信息。[使用Nginx进行代理服务和软件负载平衡](http://library.linode.com/web-servers/nginx/configuration/front-end-proxy-and-software-load-balancing)。
- Apache Web服务器，也可以用作HTTP（S）负载均衡器。

还有许多硬件负载平衡器可以路由TCP和HTTP（S）流量。

## 集群实现高性能

[CloudHub](https://docs.mulesoft.com/runtime-manager/)和[Pivotal Cloud Foundry](https://docs.mulesoft.com/runtime-manager/deploying-to-pcf) 上的高性能实现方式不同，因此本节仅适用[于内部部署](https://docs.mulesoft.com/runtime-manager/deploying-to-your-own-servers)。

如果高性能是您的主要目标（而不是可靠性），则可以使用**性能配置文件**配置Mule集群或单个应用程序以获得最佳性能。通过为群集中的特定应用程序实施性能配置文件，您可以在同一群集中部署具有不同性能和可靠性要求的应用程序时最大化部署的可伸缩性。通过在容器级别实现性能配置文件，可以将其应用于该容器中的所有应用程序。应用程序级配置会覆盖容器级配置。

设置性能配置文件有两个影响：

- 它禁用分布式队列，而是使用本地队列来阻止共享数据网格中的数据序列化/反序列化和分发。
- 它实现了没有备份的对象存储，以避免复制。

要在*容器*级别配置性能配置文件，请**mule-cluster.properties**从命令行或wrapper.conf 添加到系统属性或从系统属性添加到系统属性：

```
mule.cluster.storeprofile=performance
```

要在*单个应用程序*级别配置性能配置文件，请在配置包装器中添加配置文件，如下所示。

**性能商店档案**

```xml
<mule>
   <configuration>
      <cluster:cluster-config>
         <cluster:performance-store-profile/>
      </cluster:cluster-config>
   </configuration>
</mule>
```

请记住，应用程序级配置会覆盖容器级配置。如果您希望配置容器以获得高性能，但在该容器中使一个或多个单独的应用程序优先考虑可靠性，请在这些应用程序中包含以下代码：

**可靠的商店简介**

```xml
<mule>
    <configuration>
        <cluster:cluster-config>
            <cluster:reliable-store-profile/>
        </cluster:cluster-config>
    </configuration>
</mule>
```

如果端点不支持负载平衡的高负载，则应用性能配置文件可能会降低性能。如果您使用基于文件的传输与异步处理策略，JMS主题，多播或HTTP连接器而没有负载均衡器，则进入单个节点的大量消息可能会导致瓶颈，因此性能可能更好关闭这些应用程序的性能配置文件。

您还可以选择定义群集中所需的最小机器数，以使其保持运行状态。这可以使您获得一致性改进。在我们的[仲裁管理部分中](https://docs.mulesoft.com/mule-runtime/3.9/creating-and-managing-a-cluster-manually#quorum-management)查找更多信息。

## HA群集演示

要亲自评估Mule的HA群集功能，请继续阅读[Mule HA Demo](https://docs.mulesoft.com/mule-runtime/3.9/evaluating-mule-high-availability-clusters-demo)。Mule HA Demo Bundle旨在帮助新用户评估Mule High Availability Clusters的功能，教您如何使用Mule管理控制台创建Mule运行时集群，然后部署应用程序以在集群上运行。此外，该演示模拟了两种处理方案，说明了集群自动平衡正常处理负载的能力，以及在故障转移情况下可靠地保持活动状态的能力。

## 最佳实践

有许多与群集相关的推荐做法。这些包括：

- 尽可能将应用程序组织成一系列步骤，每个步骤将消息从一个事务存储移动到另一个事务存储。
- 如果应用程序处理来自非事务传输的消息，请使用[可靠性模式](https://docs.mulesoft.com/mule-runtime/3.9/reliability-patterns)将它们移动到事务存储（如VM或JMS存储）。
- 使用事务处理来自事务传输的消息。这可确保在遇到错误时，消息重新处理。
- 使用分布式存储（例如与VM或JMS传输一起使用的存储） - 这些存储可供整个群集使用。这对于与传输（如文件，FTP和JDBC）一起使用的非分布式存储更为可取 - 这些存储一次由单个节点读取。
- 使用VM传输可获得最佳性能。将JMS传输用于需要在整个群集退出后保存数据的应用程序。
- 创建群集中最符合您需求的节点数。
- 实施[可靠性模式](https://docs.mulesoft.com/mule-runtime/3.9/reliability-patterns)以创建高可靠性应用。

## 先决条件和限制

- 迄今为止，MuleSoft仅测试了八个节点集群。如果您计划扩展到超过八个节点。如果您计划在群集中使用许多节点，或者您计划存储大型消息有效负载，建议您在[Hazelcast文档中](https://hazelcast.org/documentation/)研究Hazelcast群集扩展，或者联系[MuleSoft支持](http://support.mulesoft.com/)
- 您必须在群集中至少有两个Mule运行时，每个运行时应在不同的物理（或虚拟）计算机上运行。
- 为了保持群集中节点之间的同步，Mule HA需要服务器之间的可靠网络连接。
- 您必须保持以下端口打开才能设置Mule集群：端口5701和端口54327。
- 由于使用多播执行新的集群成员发现，因此需要启用多播IP：224.2.2.3
- 要提供TCP请求，需要在Mule集群之间进行一些负载平衡。有关可以使用的第三方负载平衡器的详细信息，请参阅[群集和负载平衡](https://docs.mulesoft.com/mule-runtime/3.9/mule-high-availability-ha-clusters#clustering-and-load-balancing)。您还可以通过将流分成一系列步骤并将每个步骤与VM等传输连接来对群集内的处理进行负载平衡。此群集启用每个步骤，允许Mule更好地平衡节点之间的负载。
- 如果[自定义消息源](https://docs.mulesoft.com/mule-runtime/3.9/endpoint-configuration-reference)未使用消息接收器定义节点[轮询](http://en.wikipedia.org/wiki/Polling_(computer_science))，则必须配置消息源以实现ClusterizableMessageSource接口。
  ClusterizableMessageSource指示集群中只有一个应用程序节点包含消息源的活动（即已启动）实例; 这是ACTIVE节点。如果活动节点发生故障，ClusterizableMessageSource将选择一个新的活动节点，然后在该节点中启动消息源。

## 也可以看看

- [安装企业许可证](https://docs.mulesoft.com/mule-runtime/3.9/installing-an-enterprise-license)以开始管理生产中的集群。
- 有关如何创建和管理群集的说明，请参阅[管理服务器](https://docs.mulesoft.com/runtime-manager/managing-servers#create-a-cluster)。